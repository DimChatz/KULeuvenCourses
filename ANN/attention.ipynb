{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Networks and Deep Learning  \n",
        "##Assignment 3.3 - Self-attention and Transformers\n",
        "\n",
        "Prof. Dr. Ir. Johan A. K. Suykens     \n",
        "\n",
        "In this file, we first understand the self-attention mechanism by implementing it both with ``NumPy`` and ``PyTorch``.\n",
        "Then, we implement a 6-layer Vision Transformer (ViT) and train it on the MNIST dataset.\n",
        "\n",
        "All training will be conducted on a single T4 GPU.\n"
      ],
      "metadata": {
        "id": "3i1KfYGFRRWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please first load your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlTJbgaaRTct",
        "outputId": "6acfea63-1302-43cf-a6df-7ad5e7357fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Qu6w5GLkRezN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76dee7c8-7453-45e9-c93c-d82bcfdaab33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 28 14:35:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-attention Mechanism\n",
        "Self-attention is the core mechanism in Transformer."
      ],
      "metadata": {
        "id": "N-sUz1A9SzVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention with NumPy\n",
        "To have a better understanding of it, we first manually implement self-attention mechanism with ``numpy``. You can check the dimension of each variable during the matrix computation.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ],
      "metadata": {
        "id": "v6ol1XZtiPpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "# I. Define the input data X\n",
        "# X consists out of 32 samples, each sample has dimensionality 256\n",
        "n = 32\n",
        "d = 256\n",
        "X = randn(n, d) # (32, 256)\n",
        "\n",
        "# II. Generate the projection weights\n",
        "Wq = randn(d, d) #(256, 256)\n",
        "Wk = randn(d, d)\n",
        "Wv = randn(d, d)\n",
        "\n",
        "# III. Project X to find its query, keys and values vectors\n",
        "Q = np.dot(X, Wq) # (32, 256)\n",
        "K = np.dot(X, Wk)\n",
        "V = np.dot(X, Wv)\n",
        "\n",
        "# IV. Compute the self-attention score, denoted by A\n",
        "# A = softmax(QK^T / \\sqrt{d})\n",
        "# Define the softmax function\n",
        "def softmax(z):\n",
        "    z = np.clip(z, 100, -100) # clip in case softmax explodes\n",
        "    tmp = np.exp(z)\n",
        "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
        "    return res\n",
        "\n",
        "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
        "\n",
        "# V. Compute the self-attention output\n",
        "# outputs = A * V\n",
        "outputs = np.dot(A, V) #(32, 256)\n",
        "\n",
        "print(\"The attention outputs are\\n {}\".format(outputs))"
      ],
      "metadata": {
        "id": "AgWIgp51RgC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7314aada-a1af-4d17-c0c5-f0ea79666ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The attention outputs are\n",
            " [[-0.1065401   2.6477586  -1.29832641 ... -2.17956357 -5.25596909\n",
            "   3.17892371]\n",
            " [-0.1065401   2.6477586  -1.29832641 ... -2.17956357 -5.25596909\n",
            "   3.17892371]\n",
            " [-0.1065401   2.6477586  -1.29832641 ... -2.17956357 -5.25596909\n",
            "   3.17892371]\n",
            " ...\n",
            " [-0.1065401   2.6477586  -1.29832641 ... -2.17956357 -5.25596909\n",
            "   3.17892371]\n",
            " [-0.1065401   2.6477586  -1.29832641 ... -2.17956357 -5.25596909\n",
            "   3.17892371]\n",
            " [-0.1065401   2.6477586  -1.29832641 ... -2.17956357 -5.25596909\n",
            "   3.17892371]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention with PyTorch\n",
        "Now, we implement self-attention with ``PyTorch``, which is commonly used when building Transformers.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ],
      "metadata": {
        "id": "iozM1k4khO0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_input, dim_q, dim_v):\n",
        "        '''\n",
        "        dim_input: the dimension of each sample\n",
        "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
        "        dim_v: dimension of V matrix, also the  dimension of the attention output\n",
        "        '''\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_q = dim_q\n",
        "        self.dim_k = dim_q\n",
        "        self.dim_v = dim_v\n",
        "\n",
        "        # Define the linear projection\n",
        "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
        "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
        "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
        "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_q = x.shape\n",
        "\n",
        "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
        "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
        "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
        "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
        "\n",
        "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "        print('attention matrix: ', dist.shape)\n",
        "\n",
        "        outputs = torch.bmm(dist, v)\n",
        "        print('attention outputs: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "batch_size = 32 # number of samples in a batch\n",
        "dim_input = 128 # dimension of each item in the sample sequence\n",
        "seq_len = 20 # sequence length for each sample\n",
        "x = torch.randn(batch_size, seq_len, dim_input)\n",
        "self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 48)\n",
        "\n",
        "attention = self_attention(x)\n",
        "\n",
        "#print(attention)"
      ],
      "metadata": {
        "id": "qng07v8xdaPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65da008-c27b-4d89-ca09-d136c38099e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape:torch.Size([32, 20, 128]) \n",
            " Q.shape:torch.Size([32, 20, 64]) \n",
            " K.shape:torch.Size([32, 20, 64]) \n",
            " V.shape:torch.Size([32, 20, 48])\n",
            "attention matrix:  torch.Size([32, 20, 20])\n",
            "attention outputs:  torch.Size([32, 20, 48])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "In this section, we implement a 6-layer Vision Transformer (ViT) and trained it on the MNIST dataset.\n",
        "We consider the classification tasks.\n",
        "First, we load the MNIST dataset as follows:"
      ],
      "metadata": {
        "id": "WZaAFL8MS2Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "rZ-eIaeZjWjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This package is needed to build the transformer\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "-C06IoPIjePg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ca4367-59c3-4531-cccc-a4f6ba3fe8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build ViT from scratch\n",
        "Recall that each Transformer block include 2 modules: the self-attention module, the feedforward module."
      ],
      "metadata": {
        "id": "wx9eZrMpmA2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(), # Gaussian Error Linear Units is another type of activation function\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "Vr6d7IWfjpxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and test function\n"
      ],
      "metadata": {
        "id": "YTawNC64mhBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "rKJ4tjCjjycH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # We do not need to remember the gradients when testing\n",
        "    # This will help reduce memory\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('Average test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)')"
      ],
      "metadata": {
        "id": "vph2CrNxj6ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start training!\n",
        "Here, you can change the ViT structure by changing the hyper-parametrs inside ``ViT`` function.\n",
        "The default settings are with 6 layers, 8 heads for the multi-head attention mechanism and embedding dimension of 64.\n",
        "You can also increase the number of epochs to obtain better results."
      ],
      "metadata": {
        "id": "DRYys50km0-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "dim = 64\n",
        "depth = 6\n",
        "heads = 8\n",
        "mlp_dim = 128\n",
        "for dim in [32, 64, 128]:\n",
        "    # You can change the architecture here\n",
        "    model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "                dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim)\n",
        "    model = model.cuda()\n",
        "    # We also print the network architecture\n",
        "    #model\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_loss_history, test_loss_history = [], []\n",
        "    N_EPOCHS = 50\n",
        "\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "    # Gradually reduce the learning rate while training\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.912)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        #print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "        train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "        evaluate(model, test_loader, test_loss_history)\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9c2VR2sW7jj",
        "outputId": "979241b5-5d46-45c0-d20c-d82c9dd476ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test loss: 0.2635  Accuracy: 9202/10000 (92.02%)\n",
            "Average test loss: 0.1715  Accuracy: 9457/10000 (94.57%)\n",
            "Average test loss: 0.1334  Accuracy: 9582/10000 (95.82%)\n",
            "Average test loss: 0.1111  Accuracy: 9662/10000 (96.62%)\n",
            "Average test loss: 0.0932  Accuracy: 9720/10000 (97.20%)\n",
            "Average test loss: 0.0871  Accuracy: 9725/10000 (97.25%)\n",
            "Average test loss: 0.0908  Accuracy: 9723/10000 (97.23%)\n",
            "Average test loss: 0.0855  Accuracy: 9734/10000 (97.34%)\n",
            "Average test loss: 0.0785  Accuracy: 9752/10000 (97.52%)\n",
            "Average test loss: 0.0749  Accuracy: 9776/10000 (97.76%)\n",
            "Average test loss: 0.0681  Accuracy: 9787/10000 (97.87%)\n",
            "Average test loss: 0.0724  Accuracy: 9776/10000 (97.76%)\n",
            "Average test loss: 0.0730  Accuracy: 9798/10000 (97.98%)\n",
            "Average test loss: 0.0709  Accuracy: 9796/10000 (97.96%)\n",
            "Average test loss: 0.0673  Accuracy: 9814/10000 (98.14%)\n",
            "Average test loss: 0.0678  Accuracy: 9816/10000 (98.16%)\n",
            "Average test loss: 0.0667  Accuracy: 9820/10000 (98.20%)\n",
            "Average test loss: 0.0679  Accuracy: 9803/10000 (98.03%)\n",
            "Average test loss: 0.0691  Accuracy: 9826/10000 (98.26%)\n",
            "Average test loss: 0.0709  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.0735  Accuracy: 9827/10000 (98.27%)\n",
            "Average test loss: 0.0747  Accuracy: 9819/10000 (98.19%)\n",
            "Average test loss: 0.0787  Accuracy: 9825/10000 (98.25%)\n",
            "Average test loss: 0.0751  Accuracy: 9830/10000 (98.30%)\n",
            "Average test loss: 0.0754  Accuracy: 9828/10000 (98.28%)\n",
            "Average test loss: 0.0767  Accuracy: 9834/10000 (98.34%)\n",
            "Average test loss: 0.0843  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.0805  Accuracy: 9832/10000 (98.32%)\n",
            "Average test loss: 0.0833  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.0839  Accuracy: 9825/10000 (98.25%)\n",
            "Average test loss: 0.0848  Accuracy: 9832/10000 (98.32%)\n",
            "Average test loss: 0.0874  Accuracy: 9824/10000 (98.24%)\n",
            "Average test loss: 0.0904  Accuracy: 9829/10000 (98.29%)\n",
            "Average test loss: 0.0891  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.0928  Accuracy: 9826/10000 (98.26%)\n",
            "Average test loss: 0.0908  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.0939  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.0956  Accuracy: 9819/10000 (98.19%)\n",
            "Average test loss: 0.0972  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.0972  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.0988  Accuracy: 9826/10000 (98.26%)\n",
            "Average test loss: 0.0998  Accuracy: 9819/10000 (98.19%)\n",
            "Average test loss: 0.0999  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.1019  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.1024  Accuracy: 9823/10000 (98.23%)\n",
            "Average test loss: 0.1034  Accuracy: 9819/10000 (98.19%)\n",
            "Average test loss: 0.1052  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.1051  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.1058  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.1072  Accuracy: 9814/10000 (98.14%)\n",
            "Execution time: 716.90 seconds\n",
            "Average test loss: 0.2095  Accuracy: 9320/10000 (93.20%)\n",
            "Average test loss: 0.1110  Accuracy: 9643/10000 (96.43%)\n",
            "Average test loss: 0.0991  Accuracy: 9694/10000 (96.94%)\n",
            "Average test loss: 0.0880  Accuracy: 9715/10000 (97.15%)\n",
            "Average test loss: 0.0626  Accuracy: 9801/10000 (98.01%)\n",
            "Average test loss: 0.0603  Accuracy: 9812/10000 (98.12%)\n",
            "Average test loss: 0.0602  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.0638  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.0626  Accuracy: 9824/10000 (98.24%)\n",
            "Average test loss: 0.0565  Accuracy: 9831/10000 (98.31%)\n",
            "Average test loss: 0.0571  Accuracy: 9839/10000 (98.39%)\n",
            "Average test loss: 0.0616  Accuracy: 9837/10000 (98.37%)\n",
            "Average test loss: 0.0586  Accuracy: 9833/10000 (98.33%)\n",
            "Average test loss: 0.0619  Accuracy: 9836/10000 (98.36%)\n",
            "Average test loss: 0.0681  Accuracy: 9833/10000 (98.33%)\n",
            "Average test loss: 0.0615  Accuracy: 9841/10000 (98.41%)\n",
            "Average test loss: 0.0621  Accuracy: 9854/10000 (98.54%)\n",
            "Average test loss: 0.0615  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0625  Accuracy: 9850/10000 (98.50%)\n",
            "Average test loss: 0.0734  Accuracy: 9833/10000 (98.33%)\n",
            "Average test loss: 0.0602  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0765  Accuracy: 9838/10000 (98.38%)\n",
            "Average test loss: 0.0603  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0626  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0661  Accuracy: 9868/10000 (98.68%)\n",
            "Average test loss: 0.0660  Accuracy: 9865/10000 (98.65%)\n",
            "Average test loss: 0.0668  Accuracy: 9868/10000 (98.68%)\n",
            "Average test loss: 0.0674  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0685  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0697  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0704  Accuracy: 9870/10000 (98.70%)\n",
            "Average test loss: 0.0717  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0725  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0735  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0744  Accuracy: 9870/10000 (98.70%)\n",
            "Average test loss: 0.0754  Accuracy: 9868/10000 (98.68%)\n",
            "Average test loss: 0.0767  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0776  Accuracy: 9871/10000 (98.71%)\n",
            "Average test loss: 0.0785  Accuracy: 9871/10000 (98.71%)\n",
            "Average test loss: 0.0797  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0812  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0816  Accuracy: 9871/10000 (98.71%)\n",
            "Average test loss: 0.0831  Accuracy: 9872/10000 (98.72%)\n",
            "Average test loss: 0.0840  Accuracy: 9872/10000 (98.72%)\n",
            "Average test loss: 0.0854  Accuracy: 9871/10000 (98.71%)\n",
            "Average test loss: 0.0862  Accuracy: 9870/10000 (98.70%)\n",
            "Average test loss: 0.0869  Accuracy: 9872/10000 (98.72%)\n",
            "Average test loss: 0.0880  Accuracy: 9868/10000 (98.68%)\n",
            "Average test loss: 0.0891  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0901  Accuracy: 9869/10000 (98.69%)\n",
            "Execution time: 803.65 seconds\n",
            "Average test loss: 0.1577  Accuracy: 9506/10000 (95.06%)\n",
            "Average test loss: 0.1052  Accuracy: 9668/10000 (96.68%)\n",
            "Average test loss: 0.0703  Accuracy: 9763/10000 (97.63%)\n",
            "Average test loss: 0.0851  Accuracy: 9730/10000 (97.30%)\n",
            "Average test loss: 0.0800  Accuracy: 9754/10000 (97.54%)\n",
            "Average test loss: 0.0564  Accuracy: 9829/10000 (98.29%)\n",
            "Average test loss: 0.0633  Accuracy: 9829/10000 (98.29%)\n",
            "Average test loss: 0.0585  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.0675  Accuracy: 9824/10000 (98.24%)\n",
            "Average test loss: 0.0586  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0575  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0577  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.0576  Accuracy: 9861/10000 (98.61%)\n",
            "Average test loss: 0.0626  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0578  Accuracy: 9860/10000 (98.60%)\n",
            "Average test loss: 0.0521  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0600  Accuracy: 9863/10000 (98.63%)\n",
            "Average test loss: 0.0551  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0634  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0586  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0573  Accuracy: 9882/10000 (98.82%)\n",
            "Average test loss: 0.0582  Accuracy: 9881/10000 (98.81%)\n",
            "Average test loss: 0.0596  Accuracy: 9880/10000 (98.80%)\n",
            "Average test loss: 0.0618  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0623  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0629  Accuracy: 9873/10000 (98.73%)\n",
            "Average test loss: 0.0635  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0641  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0646  Accuracy: 9878/10000 (98.78%)\n",
            "Average test loss: 0.0654  Accuracy: 9878/10000 (98.78%)\n",
            "Average test loss: 0.0661  Accuracy: 9878/10000 (98.78%)\n",
            "Average test loss: 0.0668  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0675  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0683  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0690  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0699  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0706  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0713  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0722  Accuracy: 9879/10000 (98.79%)\n",
            "Average test loss: 0.0731  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0739  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0747  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0755  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0763  Accuracy: 9879/10000 (98.79%)\n",
            "Average test loss: 0.0772  Accuracy: 9879/10000 (98.79%)\n",
            "Average test loss: 0.0779  Accuracy: 9880/10000 (98.80%)\n",
            "Average test loss: 0.0788  Accuracy: 9883/10000 (98.83%)\n",
            "Average test loss: 0.0798  Accuracy: 9880/10000 (98.80%)\n",
            "Average test loss: 0.0804  Accuracy: 9881/10000 (98.81%)\n",
            "Average test loss: 0.0813  Accuracy: 9880/10000 (98.80%)\n",
            "Execution time: 759.70 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "dim = 64\n",
        "depth = 6\n",
        "heads = 8\n",
        "mlp_dim = 128\n",
        "for depth in [3, 6, 12]:\n",
        "    # You can change the architecture here\n",
        "    model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "                dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim)\n",
        "    model = model.cuda()\n",
        "    # We also print the network architecture\n",
        "    #model\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_loss_history, test_loss_history = [], []\n",
        "    N_EPOCHS = 50\n",
        "\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "    # Gradually reduce the learning rate while training\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.912)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        #print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "        train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "        evaluate(model, test_loader, test_loss_history)\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i03l3KYOuLSJ",
        "outputId": "8270370b-be3a-4d85-bb2b-b8f10efdae01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test loss: 0.1881  Accuracy: 9403/10000 (94.03%)\n",
            "Average test loss: 0.1395  Accuracy: 9559/10000 (95.59%)\n",
            "Average test loss: 0.0989  Accuracy: 9679/10000 (96.79%)\n",
            "Average test loss: 0.1016  Accuracy: 9682/10000 (96.82%)\n",
            "Average test loss: 0.0867  Accuracy: 9725/10000 (97.25%)\n",
            "Average test loss: 0.0736  Accuracy: 9767/10000 (97.67%)\n",
            "Average test loss: 0.0716  Accuracy: 9766/10000 (97.66%)\n",
            "Average test loss: 0.0701  Accuracy: 9786/10000 (97.86%)\n",
            "Average test loss: 0.0689  Accuracy: 9790/10000 (97.90%)\n",
            "Average test loss: 0.0716  Accuracy: 9798/10000 (97.98%)\n",
            "Average test loss: 0.0716  Accuracy: 9800/10000 (98.00%)\n",
            "Average test loss: 0.0734  Accuracy: 9797/10000 (97.97%)\n",
            "Average test loss: 0.0694  Accuracy: 9801/10000 (98.01%)\n",
            "Average test loss: 0.0766  Accuracy: 9785/10000 (97.85%)\n",
            "Average test loss: 0.0685  Accuracy: 9836/10000 (98.36%)\n",
            "Average test loss: 0.0745  Accuracy: 9825/10000 (98.25%)\n",
            "Average test loss: 0.0753  Accuracy: 9813/10000 (98.13%)\n",
            "Average test loss: 0.0789  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.0783  Accuracy: 9827/10000 (98.27%)\n",
            "Average test loss: 0.0833  Accuracy: 9809/10000 (98.09%)\n",
            "Average test loss: 0.0817  Accuracy: 9827/10000 (98.27%)\n",
            "Average test loss: 0.0885  Accuracy: 9825/10000 (98.25%)\n",
            "Average test loss: 0.0886  Accuracy: 9813/10000 (98.13%)\n",
            "Average test loss: 0.0871  Accuracy: 9823/10000 (98.23%)\n",
            "Average test loss: 0.0932  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.0929  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.0942  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.0960  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.1007  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.1178  Accuracy: 9806/10000 (98.06%)\n",
            "Average test loss: 0.1038  Accuracy: 9812/10000 (98.12%)\n",
            "Average test loss: 0.1033  Accuracy: 9819/10000 (98.19%)\n",
            "Average test loss: 0.1043  Accuracy: 9814/10000 (98.14%)\n",
            "Average test loss: 0.1055  Accuracy: 9813/10000 (98.13%)\n",
            "Average test loss: 0.1063  Accuracy: 9816/10000 (98.16%)\n",
            "Average test loss: 0.1071  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.1089  Accuracy: 9815/10000 (98.15%)\n",
            "Average test loss: 0.1099  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.1115  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.1126  Accuracy: 9820/10000 (98.20%)\n",
            "Average test loss: 0.1140  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.1156  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.1167  Accuracy: 9820/10000 (98.20%)\n",
            "Average test loss: 0.1184  Accuracy: 9819/10000 (98.19%)\n",
            "Average test loss: 0.1198  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.1211  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.1224  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.1236  Accuracy: 9819/10000 (98.19%)\n",
            "Average test loss: 0.1246  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.1266  Accuracy: 9817/10000 (98.17%)\n",
            "Execution time: 582.28 seconds\n",
            "Average test loss: 0.2562  Accuracy: 9190/10000 (91.90%)\n",
            "Average test loss: 0.1155  Accuracy: 9634/10000 (96.34%)\n",
            "Average test loss: 0.1092  Accuracy: 9654/10000 (96.54%)\n",
            "Average test loss: 0.0798  Accuracy: 9745/10000 (97.45%)\n",
            "Average test loss: 0.0697  Accuracy: 9774/10000 (97.74%)\n",
            "Average test loss: 0.0757  Accuracy: 9755/10000 (97.55%)\n",
            "Average test loss: 0.0663  Accuracy: 9796/10000 (97.96%)\n",
            "Average test loss: 0.0726  Accuracy: 9776/10000 (97.76%)\n",
            "Average test loss: 0.0717  Accuracy: 9780/10000 (97.80%)\n",
            "Average test loss: 0.0636  Accuracy: 9818/10000 (98.18%)\n",
            "Average test loss: 0.0690  Accuracy: 9805/10000 (98.05%)\n",
            "Average test loss: 0.0631  Accuracy: 9824/10000 (98.24%)\n",
            "Average test loss: 0.0671  Accuracy: 9824/10000 (98.24%)\n",
            "Average test loss: 0.0790  Accuracy: 9805/10000 (98.05%)\n",
            "Average test loss: 0.0666  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0765  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.0660  Accuracy: 9857/10000 (98.57%)\n",
            "Average test loss: 0.0802  Accuracy: 9828/10000 (98.28%)\n",
            "Average test loss: 0.0763  Accuracy: 9832/10000 (98.32%)\n",
            "Average test loss: 0.0762  Accuracy: 9841/10000 (98.41%)\n",
            "Average test loss: 0.0839  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.0835  Accuracy: 9827/10000 (98.27%)\n",
            "Average test loss: 0.0778  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0776  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0781  Accuracy: 9850/10000 (98.50%)\n",
            "Average test loss: 0.0787  Accuracy: 9850/10000 (98.50%)\n",
            "Average test loss: 0.0802  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.0814  Accuracy: 9854/10000 (98.54%)\n",
            "Average test loss: 0.0830  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0840  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0863  Accuracy: 9850/10000 (98.50%)\n",
            "Average test loss: 0.0949  Accuracy: 9841/10000 (98.41%)\n",
            "Average test loss: 0.0894  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0875  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0879  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0882  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0891  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0896  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0903  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0912  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0918  Accuracy: 9848/10000 (98.48%)\n",
            "Average test loss: 0.0928  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0933  Accuracy: 9848/10000 (98.48%)\n",
            "Average test loss: 0.0942  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.0952  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0959  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0969  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0977  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.0988  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0995  Accuracy: 9847/10000 (98.47%)\n",
            "Execution time: 738.63 seconds\n",
            "Average test loss: 0.2367  Accuracy: 9255/10000 (92.55%)\n",
            "Average test loss: 0.1356  Accuracy: 9579/10000 (95.79%)\n",
            "Average test loss: 0.0918  Accuracy: 9722/10000 (97.22%)\n",
            "Average test loss: 0.0831  Accuracy: 9741/10000 (97.41%)\n",
            "Average test loss: 0.0791  Accuracy: 9751/10000 (97.51%)\n",
            "Average test loss: 0.0664  Accuracy: 9791/10000 (97.91%)\n",
            "Average test loss: 0.0706  Accuracy: 9783/10000 (97.83%)\n",
            "Average test loss: 0.0656  Accuracy: 9802/10000 (98.02%)\n",
            "Average test loss: 0.0619  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.0679  Accuracy: 9806/10000 (98.06%)\n",
            "Average test loss: 0.0625  Accuracy: 9830/10000 (98.30%)\n",
            "Average test loss: 0.0758  Accuracy: 9801/10000 (98.01%)\n",
            "Average test loss: 0.0686  Accuracy: 9823/10000 (98.23%)\n",
            "Average test loss: 0.0676  Accuracy: 9850/10000 (98.50%)\n",
            "Average test loss: 0.0708  Accuracy: 9834/10000 (98.34%)\n",
            "Average test loss: 0.0656  Accuracy: 9859/10000 (98.59%)\n",
            "Average test loss: 0.0717  Accuracy: 9858/10000 (98.58%)\n",
            "Average test loss: 0.0724  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0731  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.0818  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0789  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.0765  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.0713  Accuracy: 9868/10000 (98.68%)\n",
            "Average test loss: 0.0708  Accuracy: 9871/10000 (98.71%)\n",
            "Average test loss: 0.0716  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0726  Accuracy: 9871/10000 (98.71%)\n",
            "Average test loss: 0.0731  Accuracy: 9872/10000 (98.72%)\n",
            "Average test loss: 0.0741  Accuracy: 9873/10000 (98.73%)\n",
            "Average test loss: 0.0749  Accuracy: 9872/10000 (98.72%)\n",
            "Average test loss: 0.0758  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0766  Accuracy: 9873/10000 (98.73%)\n",
            "Average test loss: 0.0775  Accuracy: 9873/10000 (98.73%)\n",
            "Average test loss: 0.0785  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0794  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0805  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0815  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0823  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0833  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0848  Accuracy: 9873/10000 (98.73%)\n",
            "Average test loss: 0.0855  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0867  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0878  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0888  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0900  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0909  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0920  Accuracy: 9873/10000 (98.73%)\n",
            "Average test loss: 0.0931  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0940  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0953  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0964  Accuracy: 9875/10000 (98.75%)\n",
            "Execution time: 1113.92 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "dim = 64\n",
        "depth = 6\n",
        "heads = 8\n",
        "mlp_dim = 128\n",
        "for heads in [4, 8, 16]:\n",
        "    # You can change the architecture here\n",
        "    model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "                dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim)\n",
        "    model = model.cuda()\n",
        "    # We also print the network architecture\n",
        "    #model\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_loss_history, test_loss_history = [], []\n",
        "    N_EPOCHS = 50\n",
        "\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "    # Gradually reduce the learning rate while training\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.912)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        #print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "        train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "        evaluate(model, test_loader, test_loss_history)\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "metadata": {
        "id": "XicoRf8_nUTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70c3541-e4ab-4712-fe4b-574f6900a466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test loss: 0.1797  Accuracy: 9415/10000 (94.15%)\n",
            "Average test loss: 0.1194  Accuracy: 9624/10000 (96.24%)\n",
            "Average test loss: 0.0997  Accuracy: 9684/10000 (96.84%)\n",
            "Average test loss: 0.0936  Accuracy: 9711/10000 (97.11%)\n",
            "Average test loss: 0.0692  Accuracy: 9793/10000 (97.93%)\n",
            "Average test loss: 0.0699  Accuracy: 9788/10000 (97.88%)\n",
            "Average test loss: 0.0671  Accuracy: 9795/10000 (97.95%)\n",
            "Average test loss: 0.0669  Accuracy: 9791/10000 (97.91%)\n",
            "Average test loss: 0.0712  Accuracy: 9780/10000 (97.80%)\n",
            "Average test loss: 0.0661  Accuracy: 9815/10000 (98.15%)\n",
            "Average test loss: 0.0820  Accuracy: 9780/10000 (97.80%)\n",
            "Average test loss: 0.0676  Accuracy: 9825/10000 (98.25%)\n",
            "Average test loss: 0.0663  Accuracy: 9830/10000 (98.30%)\n",
            "Average test loss: 0.0746  Accuracy: 9803/10000 (98.03%)\n",
            "Average test loss: 0.0700  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.0731  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.0647  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.0715  Accuracy: 9833/10000 (98.33%)\n",
            "Average test loss: 0.0719  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0715  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0738  Accuracy: 9848/10000 (98.48%)\n",
            "Average test loss: 0.0753  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.1041  Accuracy: 9805/10000 (98.05%)\n",
            "Average test loss: 0.0804  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.0770  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.0764  Accuracy: 9848/10000 (98.48%)\n",
            "Average test loss: 0.0774  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0788  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0803  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.0817  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0831  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0848  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0863  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0872  Accuracy: 9854/10000 (98.54%)\n",
            "Average test loss: 0.0886  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0900  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0918  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.0931  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0950  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.0956  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0973  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0987  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.1008  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.1015  Accuracy: 9854/10000 (98.54%)\n",
            "Average test loss: 0.1030  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.1044  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.1054  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.1069  Accuracy: 9856/10000 (98.56%)\n",
            "Average test loss: 0.1079  Accuracy: 9856/10000 (98.56%)\n",
            "Average test loss: 0.1095  Accuracy: 9856/10000 (98.56%)\n",
            "Execution time: 731.06 seconds\n",
            "Average test loss: 0.1877  Accuracy: 9392/10000 (93.92%)\n",
            "Average test loss: 0.1259  Accuracy: 9595/10000 (95.95%)\n",
            "Average test loss: 0.0960  Accuracy: 9689/10000 (96.89%)\n",
            "Average test loss: 0.1027  Accuracy: 9684/10000 (96.84%)\n",
            "Average test loss: 0.0817  Accuracy: 9742/10000 (97.42%)\n",
            "Average test loss: 0.0823  Accuracy: 9742/10000 (97.42%)\n",
            "Average test loss: 0.0746  Accuracy: 9768/10000 (97.68%)\n",
            "Average test loss: 0.0705  Accuracy: 9783/10000 (97.83%)\n",
            "Average test loss: 0.0763  Accuracy: 9784/10000 (97.84%)\n",
            "Average test loss: 0.0706  Accuracy: 9796/10000 (97.96%)\n",
            "Average test loss: 0.0699  Accuracy: 9810/10000 (98.10%)\n",
            "Average test loss: 0.0712  Accuracy: 9812/10000 (98.12%)\n",
            "Average test loss: 0.0665  Accuracy: 9831/10000 (98.31%)\n",
            "Average test loss: 0.0631  Accuracy: 9839/10000 (98.39%)\n",
            "Average test loss: 0.0684  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.0765  Accuracy: 9834/10000 (98.34%)\n",
            "Average test loss: 0.0813  Accuracy: 9832/10000 (98.32%)\n",
            "Average test loss: 0.0859  Accuracy: 9815/10000 (98.15%)\n",
            "Average test loss: 0.0801  Accuracy: 9836/10000 (98.36%)\n",
            "Average test loss: 0.0745  Accuracy: 9850/10000 (98.50%)\n",
            "Average test loss: 0.0755  Accuracy: 9857/10000 (98.57%)\n",
            "Average test loss: 0.0766  Accuracy: 9860/10000 (98.60%)\n",
            "Average test loss: 0.0794  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.0806  Accuracy: 9864/10000 (98.64%)\n",
            "Average test loss: 0.0832  Accuracy: 9857/10000 (98.57%)\n",
            "Average test loss: 0.0840  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.0857  Accuracy: 9854/10000 (98.54%)\n",
            "Average test loss: 0.0877  Accuracy: 9859/10000 (98.59%)\n",
            "Average test loss: 0.0891  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0907  Accuracy: 9854/10000 (98.54%)\n",
            "Average test loss: 0.0928  Accuracy: 9856/10000 (98.56%)\n",
            "Average test loss: 0.0942  Accuracy: 9854/10000 (98.54%)\n",
            "Average test loss: 0.0959  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.0977  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.0987  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.1008  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.1027  Accuracy: 9848/10000 (98.48%)\n",
            "Average test loss: 0.1043  Accuracy: 9850/10000 (98.50%)\n",
            "Average test loss: 0.1059  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.1083  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.1097  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.1109  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.1127  Accuracy: 9853/10000 (98.53%)\n",
            "Average test loss: 0.1138  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.1160  Accuracy: 9851/10000 (98.51%)\n",
            "Average test loss: 0.1176  Accuracy: 9852/10000 (98.52%)\n",
            "Average test loss: 0.1192  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.1207  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.1213  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.1227  Accuracy: 9851/10000 (98.51%)\n",
            "Execution time: 746.18 seconds\n",
            "Average test loss: 0.1879  Accuracy: 9424/10000 (94.24%)\n",
            "Average test loss: 0.1237  Accuracy: 9633/10000 (96.33%)\n",
            "Average test loss: 0.1010  Accuracy: 9704/10000 (97.04%)\n",
            "Average test loss: 0.0883  Accuracy: 9712/10000 (97.12%)\n",
            "Average test loss: 0.0805  Accuracy: 9745/10000 (97.45%)\n",
            "Average test loss: 0.0736  Accuracy: 9765/10000 (97.65%)\n",
            "Average test loss: 0.0658  Accuracy: 9813/10000 (98.13%)\n",
            "Average test loss: 0.0700  Accuracy: 9808/10000 (98.08%)\n",
            "Average test loss: 0.0636  Accuracy: 9820/10000 (98.20%)\n",
            "Average test loss: 0.0634  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.0653  Accuracy: 9828/10000 (98.28%)\n",
            "Average test loss: 0.0634  Accuracy: 9827/10000 (98.27%)\n",
            "Average test loss: 0.0708  Accuracy: 9822/10000 (98.22%)\n",
            "Average test loss: 0.0692  Accuracy: 9836/10000 (98.36%)\n",
            "Average test loss: 0.0769  Accuracy: 9808/10000 (98.08%)\n",
            "Average test loss: 0.0711  Accuracy: 9827/10000 (98.27%)\n",
            "Average test loss: 0.0765  Accuracy: 9836/10000 (98.36%)\n",
            "Average test loss: 0.0780  Accuracy: 9826/10000 (98.26%)\n",
            "Average test loss: 0.0745  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0806  Accuracy: 9832/10000 (98.32%)\n",
            "Average test loss: 0.0773  Accuracy: 9841/10000 (98.41%)\n",
            "Average test loss: 0.0876  Accuracy: 9826/10000 (98.26%)\n",
            "Average test loss: 0.0863  Accuracy: 9833/10000 (98.33%)\n",
            "Average test loss: 0.0762  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.0786  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.0800  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.0811  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.0823  Accuracy: 9841/10000 (98.41%)\n",
            "Average test loss: 0.0838  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.0852  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.0864  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0881  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.0895  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.0907  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0917  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0935  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.0946  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.0971  Accuracy: 9841/10000 (98.41%)\n",
            "Average test loss: 0.0982  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0991  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.1007  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.1016  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.1041  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.1052  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.1064  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.1085  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.1096  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.1112  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.1121  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.1136  Accuracy: 9842/10000 (98.42%)\n",
            "Execution time: 734.54 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "dim = 64\n",
        "depth = 6\n",
        "heads = 8\n",
        "mlp_dim = 128\n",
        "for mlp_dim in [64, 128, 256]:\n",
        "    # You can change the architecture here\n",
        "    model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "                dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim)\n",
        "    model = model.cuda()\n",
        "    # We also print the network architecture\n",
        "    #model\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_loss_history, test_loss_history = [], []\n",
        "    N_EPOCHS = 50\n",
        "\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "    # Gradually reduce the learning rate while training\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.912)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        #print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "        train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "        evaluate(model, test_loader, test_loss_history)\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf7YHTxGJ0a6",
        "outputId": "b4beed70-831e-48ee-b31c-8e255d6915d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test loss: 0.2114  Accuracy: 9316/10000 (93.16%)\n",
            "Average test loss: 0.1254  Accuracy: 9606/10000 (96.06%)\n",
            "Average test loss: 0.1224  Accuracy: 9607/10000 (96.07%)\n",
            "Average test loss: 0.0901  Accuracy: 9725/10000 (97.25%)\n",
            "Average test loss: 0.0947  Accuracy: 9708/10000 (97.08%)\n",
            "Average test loss: 0.0816  Accuracy: 9733/10000 (97.33%)\n",
            "Average test loss: 0.0759  Accuracy: 9779/10000 (97.79%)\n",
            "Average test loss: 0.0673  Accuracy: 9807/10000 (98.07%)\n",
            "Average test loss: 0.0644  Accuracy: 9806/10000 (98.06%)\n",
            "Average test loss: 0.0685  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.0675  Accuracy: 9816/10000 (98.16%)\n",
            "Average test loss: 0.0689  Accuracy: 9799/10000 (97.99%)\n",
            "Average test loss: 0.0703  Accuracy: 9827/10000 (98.27%)\n",
            "Average test loss: 0.0744  Accuracy: 9820/10000 (98.20%)\n",
            "Average test loss: 0.0713  Accuracy: 9816/10000 (98.16%)\n",
            "Average test loss: 0.0791  Accuracy: 9821/10000 (98.21%)\n",
            "Average test loss: 0.0751  Accuracy: 9816/10000 (98.16%)\n",
            "Average test loss: 0.0799  Accuracy: 9824/10000 (98.24%)\n",
            "Average test loss: 0.0799  Accuracy: 9815/10000 (98.15%)\n",
            "Average test loss: 0.0749  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.0777  Accuracy: 9839/10000 (98.39%)\n",
            "Average test loss: 0.0779  Accuracy: 9840/10000 (98.40%)\n",
            "Average test loss: 0.0793  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.0797  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0807  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0819  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0836  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.0843  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0856  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.0877  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0886  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0896  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.0911  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.0922  Accuracy: 9848/10000 (98.48%)\n",
            "Average test loss: 0.0938  Accuracy: 9848/10000 (98.48%)\n",
            "Average test loss: 0.0963  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.0967  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.0982  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.0993  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.1002  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.1027  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.1034  Accuracy: 9844/10000 (98.44%)\n",
            "Average test loss: 0.1044  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.1060  Accuracy: 9842/10000 (98.42%)\n",
            "Average test loss: 0.1074  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.1086  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.1091  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.1105  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.1116  Accuracy: 9846/10000 (98.46%)\n",
            "Average test loss: 0.1124  Accuracy: 9842/10000 (98.42%)\n",
            "Execution time: 719.01 seconds\n",
            "Average test loss: 0.1979  Accuracy: 9365/10000 (93.65%)\n",
            "Average test loss: 0.1155  Accuracy: 9636/10000 (96.36%)\n",
            "Average test loss: 0.0833  Accuracy: 9738/10000 (97.38%)\n",
            "Average test loss: 0.0770  Accuracy: 9748/10000 (97.48%)\n",
            "Average test loss: 0.0764  Accuracy: 9779/10000 (97.79%)\n",
            "Average test loss: 0.0703  Accuracy: 9772/10000 (97.72%)\n",
            "Average test loss: 0.0719  Accuracy: 9776/10000 (97.76%)\n",
            "Average test loss: 0.0723  Accuracy: 9761/10000 (97.61%)\n",
            "Average test loss: 0.0648  Accuracy: 9816/10000 (98.16%)\n",
            "Average test loss: 0.0736  Accuracy: 9788/10000 (97.88%)\n",
            "Average test loss: 0.0583  Accuracy: 9831/10000 (98.31%)\n",
            "Average test loss: 0.0661  Accuracy: 9817/10000 (98.17%)\n",
            "Average test loss: 0.0590  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.0590  Accuracy: 9843/10000 (98.43%)\n",
            "Average test loss: 0.0590  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.0558  Accuracy: 9865/10000 (98.65%)\n",
            "Average test loss: 0.0628  Accuracy: 9839/10000 (98.39%)\n",
            "Average test loss: 0.0614  Accuracy: 9860/10000 (98.60%)\n",
            "Average test loss: 0.0694  Accuracy: 9848/10000 (98.48%)\n",
            "Average test loss: 0.0626  Accuracy: 9862/10000 (98.62%)\n",
            "Average test loss: 0.0693  Accuracy: 9855/10000 (98.55%)\n",
            "Average test loss: 0.0719  Accuracy: 9860/10000 (98.60%)\n",
            "Average test loss: 0.0688  Accuracy: 9863/10000 (98.63%)\n",
            "Average test loss: 0.0700  Accuracy: 9866/10000 (98.66%)\n",
            "Average test loss: 0.0709  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0716  Accuracy: 9871/10000 (98.71%)\n",
            "Average test loss: 0.0727  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0743  Accuracy: 9870/10000 (98.70%)\n",
            "Average test loss: 0.0748  Accuracy: 9872/10000 (98.72%)\n",
            "Average test loss: 0.0760  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0771  Accuracy: 9870/10000 (98.70%)\n",
            "Average test loss: 0.0780  Accuracy: 9870/10000 (98.70%)\n",
            "Average test loss: 0.0791  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0800  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0813  Accuracy: 9870/10000 (98.70%)\n",
            "Average test loss: 0.0831  Accuracy: 9871/10000 (98.71%)\n",
            "Average test loss: 0.0840  Accuracy: 9868/10000 (98.68%)\n",
            "Average test loss: 0.0857  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0872  Accuracy: 9865/10000 (98.65%)\n",
            "Average test loss: 0.0877  Accuracy: 9868/10000 (98.68%)\n",
            "Average test loss: 0.0889  Accuracy: 9866/10000 (98.66%)\n",
            "Average test loss: 0.0902  Accuracy: 9865/10000 (98.65%)\n",
            "Average test loss: 0.0911  Accuracy: 9870/10000 (98.70%)\n",
            "Average test loss: 0.0929  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0942  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0952  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0964  Accuracy: 9867/10000 (98.67%)\n",
            "Average test loss: 0.0972  Accuracy: 9865/10000 (98.65%)\n",
            "Average test loss: 0.0986  Accuracy: 9866/10000 (98.66%)\n",
            "Average test loss: 0.1001  Accuracy: 9866/10000 (98.66%)\n",
            "Execution time: 714.86 seconds\n",
            "Average test loss: 0.1950  Accuracy: 9374/10000 (93.74%)\n",
            "Average test loss: 0.1202  Accuracy: 9621/10000 (96.21%)\n",
            "Average test loss: 0.1044  Accuracy: 9667/10000 (96.67%)\n",
            "Average test loss: 0.0905  Accuracy: 9717/10000 (97.17%)\n",
            "Average test loss: 0.0775  Accuracy: 9760/10000 (97.60%)\n",
            "Average test loss: 0.0628  Accuracy: 9806/10000 (98.06%)\n",
            "Average test loss: 0.0770  Accuracy: 9790/10000 (97.90%)\n",
            "Average test loss: 0.0738  Accuracy: 9767/10000 (97.67%)\n",
            "Average test loss: 0.0720  Accuracy: 9810/10000 (98.10%)\n",
            "Average test loss: 0.0676  Accuracy: 9826/10000 (98.26%)\n",
            "Average test loss: 0.0659  Accuracy: 9811/10000 (98.11%)\n",
            "Average test loss: 0.0625  Accuracy: 9837/10000 (98.37%)\n",
            "Average test loss: 0.0648  Accuracy: 9838/10000 (98.38%)\n",
            "Average test loss: 0.0624  Accuracy: 9849/10000 (98.49%)\n",
            "Average test loss: 0.0645  Accuracy: 9845/10000 (98.45%)\n",
            "Average test loss: 0.0708  Accuracy: 9836/10000 (98.36%)\n",
            "Average test loss: 0.0715  Accuracy: 9847/10000 (98.47%)\n",
            "Average test loss: 0.0753  Accuracy: 9830/10000 (98.30%)\n",
            "Average test loss: 0.0692  Accuracy: 9869/10000 (98.69%)\n",
            "Average test loss: 0.0672  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0685  Accuracy: 9876/10000 (98.76%)\n",
            "Average test loss: 0.0671  Accuracy: 9873/10000 (98.73%)\n",
            "Average test loss: 0.0697  Accuracy: 9878/10000 (98.78%)\n",
            "Average test loss: 0.0693  Accuracy: 9878/10000 (98.78%)\n",
            "Average test loss: 0.0715  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0855  Accuracy: 9837/10000 (98.37%)\n",
            "Average test loss: 0.0761  Accuracy: 9859/10000 (98.59%)\n",
            "Average test loss: 0.0743  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0745  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0751  Accuracy: 9874/10000 (98.74%)\n",
            "Average test loss: 0.0758  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0760  Accuracy: 9879/10000 (98.79%)\n",
            "Average test loss: 0.0769  Accuracy: 9879/10000 (98.79%)\n",
            "Average test loss: 0.0772  Accuracy: 9880/10000 (98.80%)\n",
            "Average test loss: 0.0785  Accuracy: 9881/10000 (98.81%)\n",
            "Average test loss: 0.0790  Accuracy: 9883/10000 (98.83%)\n",
            "Average test loss: 0.0800  Accuracy: 9882/10000 (98.82%)\n",
            "Average test loss: 0.0806  Accuracy: 9881/10000 (98.81%)\n",
            "Average test loss: 0.0818  Accuracy: 9881/10000 (98.81%)\n",
            "Average test loss: 0.0825  Accuracy: 9884/10000 (98.84%)\n",
            "Average test loss: 0.0838  Accuracy: 9882/10000 (98.82%)\n",
            "Average test loss: 0.0845  Accuracy: 9882/10000 (98.82%)\n",
            "Average test loss: 0.0858  Accuracy: 9880/10000 (98.80%)\n",
            "Average test loss: 0.0864  Accuracy: 9884/10000 (98.84%)\n",
            "Average test loss: 0.0875  Accuracy: 9883/10000 (98.83%)\n",
            "Average test loss: 0.0892  Accuracy: 9882/10000 (98.82%)\n",
            "Average test loss: 0.0901  Accuracy: 9882/10000 (98.82%)\n",
            "Average test loss: 0.0913  Accuracy: 9875/10000 (98.75%)\n",
            "Average test loss: 0.0920  Accuracy: 9877/10000 (98.77%)\n",
            "Average test loss: 0.0932  Accuracy: 9881/10000 (98.81%)\n",
            "Execution time: 720.61 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsfdllBUXKrn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}